{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data \n",
    "clean_transaction = pd.read_csv('clean_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used to delete random column that gets written in from reading in csv, you can ignore this \n",
    "clean_transaction = clean_transaction.drop(clean_transaction.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>customer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "      <th>customer_reliability</th>\n",
       "      <th>merchant_reliability</th>\n",
       "      <th>diff_previous_step</th>\n",
       "      <th>diff_previous_amount</th>\n",
       "      <th>mean_amount</th>\n",
       "      <th>diff_from_mean_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>M1888755466</td>\n",
       "      <td>2</td>\n",
       "      <td>143.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.091908</td>\n",
       "      <td>108.778092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>M1741626453</td>\n",
       "      <td>7</td>\n",
       "      <td>16.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-127.18</td>\n",
       "      <td>35.091908</td>\n",
       "      <td>-18.401908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>M1888755466</td>\n",
       "      <td>2</td>\n",
       "      <td>56.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>39.49</td>\n",
       "      <td>35.091908</td>\n",
       "      <td>21.088092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>M840466850</td>\n",
       "      <td>6</td>\n",
       "      <td>14.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.112938</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-41.44</td>\n",
       "      <td>35.091908</td>\n",
       "      <td>-20.351908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>C1000148617</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>M1823072687</td>\n",
       "      <td>0</td>\n",
       "      <td>47.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.68</td>\n",
       "      <td>35.091908</td>\n",
       "      <td>12.328092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step     customer  age  gender     merchant  category  amount  fraud  \\\n",
       "0    30  C1000148617    5       0  M1888755466         2  143.87      0   \n",
       "1    38  C1000148617    5       0  M1741626453         7   16.69      0   \n",
       "2    42  C1000148617    5       0  M1888755466         2   56.18      0   \n",
       "3    43  C1000148617    5       0   M840466850         6   14.74      0   \n",
       "4    44  C1000148617    5       0  M1823072687         0   47.42      0   \n",
       "\n",
       "   customer_reliability  merchant_reliability  diff_previous_step  \\\n",
       "0              0.007634              0.250000                 NaN   \n",
       "1              0.007634              0.371212                 8.0   \n",
       "2              0.007634              0.250000                 4.0   \n",
       "3              0.007634              0.112938                 1.0   \n",
       "4              0.007634              0.000000                 1.0   \n",
       "\n",
       "   diff_previous_amount  mean_amount  diff_from_mean_amount  \n",
       "0                   NaN    35.091908             108.778092  \n",
       "1               -127.18    35.091908             -18.401908  \n",
       "2                 39.49    35.091908              21.088092  \n",
       "3                -41.44    35.091908             -20.351908  \n",
       "4                 32.68    35.091908              12.328092  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_transaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber general conditions for the features (or the x values): \n",
    "- features cannot contain null values; either get rid of them, impute them, or create a new category for them  \n",
    "- features must be numerical (or at least represented numerically)\n",
    "- too many features can lead to overfitting or long run times, choose sparingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic way of ensuring that features satisfy general conditions \n",
    "clean_transaction = clean_transaction.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Training/Validation/Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportions that you set for your training/validation/testing sets can change the way your machine learning algorithm performs. What's the point of each set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Training__: This set is used to train your algorithm. In almost all cases, this should be a majority of your data. Some important things to note: ensure that the training set contains all labels that you are trying to predict, ensure data is balanced, etc. Think logically; would your algorithm be able to predict the labels for something it hasn't even been trained to do? The model is as good as its training. THE MODEL SHOULD ONLY EVER BE TRAINED ON THIS SET OF DATA! Don't make the mistake of training on validation or testing data. \n",
    "- __Validation__: This set is used to help you choose your algorithm. When you're comparing algorithms and how well they do, sometimes running it on a lot of different sets or a lot of times can take a lot of resources or time. The validation set is the 'fake' testing set. Test and predict using the validation set and whichever model with the best validation set will tend to be the model with the best testing predictions. THIS SET OF DATA SHOULD ONLY BE USED FOR PREDICTIONS!\n",
    "- __Testing__: This set is used to help you determine how good your algorithm actually is. This will hopefully hint at how well your algorithm will do on foreign data. THIS SET OF DATA SHOULD ONLY BE USED FOR PREDICTIONS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important tips:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you work to manipulate your training/validation/testing sets you need to _ensure_ that your labels and features  __stay properly connected__. What I mean by that is that you don't want your features to accidentally have the wrong label; either ensure data is connected by manipulating the dataframe first, using tuples, or using some other form of connected data organization.\n",
    "- Ensure that the data in your sets __are actually good__. Sometimes data is organized by labels (e.g. the first 100 data points are fraud, next 100 points are not fraud). Ensure that 1) your sets have a good variety of labels 2) your sets are a relatively good representation of the population you're trying to predict 3) you've dealt with issues such as balanced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test(df,percent_train, percent_validate):\n",
    "    '''\n",
    "    Parameters: \n",
    "        df: the dataframe that you want to split up \n",
    "        percent_train: The percentage (in decimal form) that you want of df to be training set \n",
    "        percent_validate: The percentage (in decimal form) that you want of df to be validation set\n",
    "    Returns: \n",
    "        if conditions are correct, returns df split into training, validate, and test (in that order)\n",
    "    '''\n",
    "    # assuming that you want the remainder of the data that isn't train/validate to be testing set\n",
    "    if percent_train+percent_validate == 1: \n",
    "        print('your percentages are incorrect, you have no data left to be testing set')\n",
    "        return \n",
    "    elif percent_train+percent_validate > 1:\n",
    "        print('your percentages are incorrect, your percentages add up to more than 100%')\n",
    "        return \n",
    "    else: \n",
    "        # what conditions might you want to set here? \n",
    "        # shuffle data: df = df.sample(frac=1).reset_index(drop=True)\n",
    "        # ensure data is balanced by simulating training data to be 50% fraud and 50% not fraud \n",
    "        total_n = len(df)\n",
    "        train_n = int(percent_train*total_n)\n",
    "        validate_n = train_n+int(percent_validate*total_n)\n",
    "        train_df = df.iloc[:train_n]\n",
    "        validate_df = df.iloc[train_n:validate_n]\n",
    "        test_df = df.iloc[validate_n:]\n",
    "        return train_df,validate_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction, validate_transaction, test_transaction = train_validate_test(clean_transaction,.5,.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the length of the original dataframe and the new split up \n",
    "len(clean_transaction) == len(train_transaction)+len(validate_transaction)+len(test_transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that you can change the training, validation, and testing set as much as you please. It doesn't have to be the first n% of the data, nor does each data point have to be unique (although it would probably be a smarter idea to make them unique if you think about it). That means you can go ahead and simulate a 'fake' training set of 50% fraud and 50% not fraudalent data. You can literally do whatever you want as long as these rules aren't broken:\n",
    "- training, validation, and the testing set must __all__ have the same number of features (or x values or columns)\n",
    "- MAKE SURE FOR THE FUTURE THAT YOU ARE ONLY FITTING THE MODEL ON THE TRAINING SET AND NOTHING ELSE \n",
    "- once again, make sure your features (x values) and labels (y labels) are properly organized and correctly linked together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To my knowledge, almost all machine learning algorithms only accept lists so you'll have to convert everything into proper lists. The following is example code. Make sure you understand it, but there's a high chance you'll basically just copy and paste it (the code shouldn't change too much especially if you're working with dataframes and not lists to begin with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_labels(df,list_interested_features, interested_label):\n",
    "    '''\n",
    "    Parameters:\n",
    "        df: the dataframe that you're trying to split into features and labels \n",
    "        list_interested_features: a list of column names of the df that represent the features\n",
    "        interested_label: ONE SINGULAR COLUMN NAME OF DF, the column that contains the item you're trying to predict\n",
    "    Returns:\n",
    "        returns two lists, features and labels (in that order)\n",
    "    '''\n",
    "    # ensure that the features you select are numbers \n",
    "    features = df[list_interested_features].values.tolist()\n",
    "    labels = df[interested_label].values.tolist()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = features_and_labels(train_transaction,['age','gender','category','amount'],'fraud')\n",
    "validate_X, validate_y = features_and_labels(validate_transaction,['age','gender','category','amount'],'fraud')\n",
    "test_X, test_y = features_and_labels(test_transaction,['age','gender','category','amount'],'fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the features (X) for the train, validate, and test all are the same (age, gender, category, amount). Also note that the list of features are in the order of the inputs. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0, 0.0, 7.0, 16.69]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input for the features method was [age, gender, category, amount]. Therefore, train_X will be a list of lists.\n",
    "For this first data point, 5 is the age, 0 is the gender, 7 is the category, 16.69 is the amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying these features and labels into machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import machine learning algorithms here \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general outline for using your data in machine learning algorithms is as follows: \n",
    "\n",
    "model = Algorithm(parameters of algorithm) <br>\n",
    "model.fit(trainX, trainY) <br>\n",
    "predictions = model.predict(X of set that you want predictions of) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify model = Algorithm(parameters of algorithm) (by changing the algorithm or the parameters of the algorithm) and you can modify predictions = model.predict(X of set that you want predictions of) (by changing what you want predictions of. However, __you cannot modify model.fit(trainX,trainY).__ The model __must__ be fit on the training features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of logistic regression \n",
    "logistic_mod = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bernardwong/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS SYNTAX \n",
    "logistic_mod.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = logistic_mod.predict(train_X)\n",
    "validation_predictions = logistic_mod.predict(validate_X)\n",
    "testing_predictions = logistic_mod.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input of predict is ALWAYS a features list (X). Also note that algorithm.predict() returns a list of predictions that are the same length as the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_predictions) == len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_predictions) == len(validate_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_predictions) == len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm.predict() is simply outputting the predicted labels of the set that you inputted in the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining how good the algorithm is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of measuring how good an algorithm is. While of course the standard is accuracy, high accuracy does __not__ necessarily mean the algorithm is good (especially in cases of imbalanced data). Make sure to test other accuracy tests such as BER, accuracy, precision, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions,actual_labels):\n",
    "    predictions = predictions.tolist()\n",
    "    is_equal = [] \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == actual_labels[i]:\n",
    "            is_equal.append(True)\n",
    "        else:\n",
    "            is_equal.append(False)\n",
    "    total_correct = sum(is_equal)\n",
    "    total_number = len(is_equal)\n",
    "    return total_correct/total_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945675918242934"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(training_predictions,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935735695053596"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(validation_predictions,validate_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939546172951005"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(testing_predictions,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be WARY of high accuracy. This does NOT necessarily mean the model is good. Implement things such as BER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ber(predictions,actual_labels):\n",
    "    temp_df = pd.DataFrame({'Predictions':predictions.tolist(),'Actual':actual_labels})\n",
    "    temp_df['Prediction same as Actual'] = temp_df['Predictions'] == temp_df['Actual']\n",
    "    actual_not_fraud = temp_df[temp_df['Actual']==0]\n",
    "    actual_fraud = temp_df[temp_df['Actual']==1]\n",
    "    fraud_rate = sum(actual_fraud['Prediction same as Actual'])/len(actual_fraud)\n",
    "    not_fraud_rate = sum(actual_not_fraud['Prediction same as Actual'])/len(actual_not_fraud)\n",
    "    ber = (fraud_rate+not_fraud_rate)/2\n",
    "    return ber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7878090115881129"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ber(training_predictions,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7900887098603275"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ber(validation_predictions,validate_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7840530651716171"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ber(testing_predictions,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to implement other methods that help measure different areas of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general this is the same for each different algorithms. There are multiple ways of improving your algorithm such as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improving or modifying your training set so that the model is properly fitted (by modify\n",
    "- changing the model itself or the parameters \n",
    "- changing the accuracy measures to ensure you're actually selecting the correct model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
